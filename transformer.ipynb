{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 5000])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spacy models for English and French (or another language)\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")  # Load English tokenizer\n",
    "spacy_fr = spacy.load(\"fr_core_news_sm\")  # Load French tokenizer\n",
    "\n",
    "# Tokenizers\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenize English text using spacy.\n",
    "    Args:\n",
    "        text (str): Input English sentence.\n",
    "    Returns:\n",
    "        list: List of tokens.\n",
    "    \"\"\"\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]  # Tokenize and extract text\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    \"\"\"\n",
    "    Tokenize French text using spacy.\n",
    "    Args:\n",
    "        text (str): Input French sentence.\n",
    "    Returns:\n",
    "        list: List of tokens.\n",
    "    \"\"\"\n",
    "    return [token.text for token in spacy_fr.tokenizer(text)]  # Tokenize and extract text\n",
    "\n",
    "# Vocabulary class\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Vocabulary class to handle word-to-index and index-to-word mappings.\n",
    "    \"\"\"\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        \"\"\"\n",
    "        Initialize the vocabulary.\n",
    "        Args:\n",
    "            freq_threshold (int): Minimum frequency for a word to be included in the vocabulary.\n",
    "        \"\"\"\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}  # Index to string mapping\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}  # String to index mapping\n",
    "        self.counter = Counter()  # Counter to keep track of word frequencies\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the size of the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        \"\"\"\n",
    "        Build the vocabulary from a list of sentences.\n",
    "        Args:\n",
    "            sentence_list (list): List of tokenized sentences.\n",
    "        \"\"\"\n",
    "        for sentence in sentence_list:\n",
    "            self.counter.update(sentence)  # Update word frequencies\n",
    "\n",
    "        for word, count in self.counter.items():\n",
    "            if count >= self.freq_threshold:  # Include words above the frequency threshold\n",
    "                self.stoi[word] = len(self.itos)  # Add word to string-to-index mapping\n",
    "                self.itos[len(self.itos)] = word  # Add word to index-to-string mapping\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        \"\"\"\n",
    "        Convert a list of tokens to a list of indices.\n",
    "        Args:\n",
    "            text (list): List of tokens.\n",
    "        Returns:\n",
    "            list: List of indices.\n",
    "        \"\"\"\n",
    "        return [self.stoi[word] if word in self.stoi else self.stoi[\"<UNK>\"] for word in text]  # Convert tokens to indices\n",
    "\n",
    "# Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for translation tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, en_texts, fr_texts, en_vocab, fr_vocab, seq_length):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        Args:\n",
    "            en_texts (list): List of English sentences.\n",
    "            fr_texts (list): List of French sentences.\n",
    "            en_vocab (Vocabulary): English vocabulary.\n",
    "            fr_vocab (Vocabulary): French vocabulary.\n",
    "            seq_length (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.en_texts = en_texts\n",
    "        self.fr_texts = fr_texts\n",
    "        self.en_vocab = en_vocab\n",
    "        self.fr_vocab = fr_vocab\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.en_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "        Returns:\n",
    "            tuple: (source sequence, target sequence) as tensors.\n",
    "        \"\"\"\n",
    "        en_text = self.en_texts[idx]  # Get English sentence\n",
    "        fr_text = self.fr_texts[idx]  # Get French sentence\n",
    "\n",
    "        # Tokenize and add special tokens\n",
    "        en_tokens = [\"<SOS>\"] + tokenize_en(en_text) + [\"<EOS>\"]  # Add <SOS> and <EOS> tokens\n",
    "        fr_tokens = [\"<SOS>\"] + tokenize_fr(fr_text) + [\"<EOS>\"]  # Add <SOS> and <EOS> tokens\n",
    "\n",
    "        # Convert tokens to indices\n",
    "        en_indices = self.en_vocab.numericalize(en_tokens)  # Convert English tokens to indices\n",
    "        fr_indices = self.fr_vocab.numericalize(fr_tokens)  # Convert French tokens to indices\n",
    "\n",
    "        # Pad sequences to the specified length\n",
    "        en_indices = self.pad_sequence(en_indices, self.seq_length)  # Pad English sequence\n",
    "        fr_indices = self.pad_sequence(fr_indices, self.seq_length)  # Pad French sequence\n",
    "\n",
    "        return torch.tensor(en_indices, dtype=torch.long), torch.tensor(fr_indices, dtype=torch.long)  # Convert to tensors\n",
    "\n",
    "    def pad_sequence(self, sequence, max_len):\n",
    "        \"\"\"\n",
    "        Pad or truncate a sequence to the specified length.\n",
    "        Args:\n",
    "            sequence (list): List of indices.\n",
    "            max_len (int): Maximum sequence length.\n",
    "        Returns:\n",
    "            list: Padded or truncated sequence.\n",
    "        \"\"\"\n",
    "        if len(sequence) < max_len:\n",
    "            sequence = sequence + [0] * (max_len - len(sequence))  # Pad with zeros\n",
    "        else:\n",
    "            sequence = sequence[:max_len]  # Truncate if too long\n",
    "        return sequence\n",
    "\n",
    "# Transformer model (same as before)\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer model for sequence-to-sequence tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer model.\n",
    "        Args:\n",
    "            src_vocab_size (int): Size of the source vocabulary.\n",
    "            tgt_vocab_size (int): Size of the target vocabulary.\n",
    "            d_model (int): Dimension of the model.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            num_layers (int): Number of encoder/decoder layers.\n",
    "            d_ff (int): Dimension of the feed-forward network.\n",
    "            max_seq_length (int): Maximum sequence length.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)  # Embedding for source tokens\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)  # Embedding for target tokens\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))  # Positional encoding\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])  # Encoder layers\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])  # Decoder layers\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)  # Final linear layer\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout layer\n",
    "        \n",
    "    def generate_mask(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Generate masks for source and target sequences.\n",
    "        Args:\n",
    "            src (torch.Tensor): Source sequence.\n",
    "            tgt (torch.Tensor): Target sequence.\n",
    "        Returns:\n",
    "            tuple: (source mask, target mask)\n",
    "        \"\"\"\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # Mask for padding tokens in source\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)  # Mask for padding tokens in target\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()  # Mask for future tokens\n",
    "        tgt_mask = tgt_mask & nopeak_mask  # Combine padding and future masks\n",
    "        return src_mask, tgt_mask\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "        Args:\n",
    "            src (torch.Tensor): Source sequence.\n",
    "            tgt (torch.Tensor): Target sequence.\n",
    "        Returns:\n",
    "            torch.Tensor: Model output.\n",
    "        \"\"\"\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)  # Generate masks\n",
    "        \n",
    "        src_embedded = self.dropout(self.encoder_embedding(src) + self.positional_encoding[:, :src.size(1), :])  # Embed source tokens\n",
    "        tgt_embedded = self.dropout(self.decoder_embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :])  # Embed target tokens\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)  # Pass through encoder layers\n",
    "            \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)  # Pass through decoder layers\n",
    "            \n",
    "        output = self.fc_out(dec_output)  # Final linear layer\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 5000  # Vocabulary size\n",
    "seq_length = 20  # Maximum sequence length\n",
    "batch_size = 32  # Batch size\n",
    "d_model = 512  # Model dimension\n",
    "num_heads = 8  # Number of attention heads\n",
    "num_layers = 6  # Number of encoder/decoder layers\n",
    "d_ff = 2048  # Feed-forward dimension\n",
    "dropout = 0.1  # Dropout rate\n",
    "num_epochs = 10  # Number of epochs\n",
    "learning_rate = 0.0001  # Learning rate\n",
    "\n",
    "# Sample data (replace with your dataset)\n",
    "en_texts = [\"I love programming.\", \"This is a test.\", \"How are you?\"]  # English sentences\n",
    "fr_texts = [\"J'adore la programmation.\", \"C'est un test.\", \"Comment ça va?\"]  # French sentences\n",
    "\n",
    "# Build vocabularies\n",
    "en_vocab = Vocabulary()  # English vocabulary\n",
    "fr_vocab = Vocabulary()  # French vocabulary\n",
    "en_vocab.build_vocab([tokenize_en(text) for text in en_texts])  # Build English vocabulary\n",
    "fr_vocab.build_vocab([tokenize_fr(text) for text in fr_texts])  # Build French vocabulary\n",
    "\n",
    "# Create dataset and data loader\n",
    "dataset = TranslationDataset(en_texts, fr_texts, en_vocab, fr_vocab, seq_length)  # Create dataset\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  # Create data loader\n",
    "\n",
    "# Initialize the Transformer model\n",
    "transformer = Transformer(len(en_vocab), len(fr_vocab), d_model, num_heads, num_layers, d_ff, seq_length, dropout)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Loss function (ignore padding tokens)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)  # Optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    transformer.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        output = transformer(src, tgt[:, :-1])  # Exclude the last token in the target sequence\n",
    "        loss = criterion(output.reshape(-1, len(fr_vocab)), tgt[:, 1:].reshape(-1))  # Compute loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)  # Compute average loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\miniconda3\\envs\\nlp_genai_live\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.4/12.8 MB 13.4 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.3/12.8 MB 16.1 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.8 MB 16.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 13.8 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.6/16.3 MB 14.1 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 5.2/16.3 MB 16.0 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 9.2/16.3 MB 16.3 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 11.5/16.3 MB 14.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 15.5/16.3 MB 15.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 16.3/16.3 MB 13.7 MB/s eta 0:00:00\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install torch spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_GenAI_Live",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
